{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Multilayer Perceptron.\n",
    "\n",
    "A Multilayer Perceptron (Neural Network) implementation example using\n",
    "TensorFlow library. This example is using the MNIST database of handwritten\n",
    "digits (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Links:\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Author: Saleem Ahmed\n",
    "Project: https://github.com/crazysal/MNIST-Logistic-Regression-MLP-CNN\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "# n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    \n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = multilayer_perceptron(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=28.324336378\n",
      "Epoch: 0002 cost=7.932121539\n",
      "Epoch: 0003 cost=5.924172065\n",
      "Epoch: 0004 cost=4.858874294\n",
      "Epoch: 0005 cost=4.193946858\n",
      "Epoch: 0006 cost=3.678970504\n",
      "Epoch: 0007 cost=3.307560816\n",
      "Epoch: 0008 cost=3.022878025\n",
      "Epoch: 0009 cost=2.757018665\n",
      "Epoch: 0010 cost=2.590802059\n",
      "Epoch: 0011 cost=2.385162545\n",
      "Epoch: 0012 cost=2.282708634\n",
      "Epoch: 0013 cost=2.107260968\n",
      "Epoch: 0014 cost=2.006777644\n",
      "Epoch: 0015 cost=1.888892937\n",
      "Optimization Finished!\n",
      "Accuracy of MNIST Train Data: 0.900818\n",
      "Accuracy of MNIST Validation Data: 0.8888\n",
      "Accuracy of MNIST Test Data: 0.8879\n",
      "Accuracy for USPS Numerals: 0.321866\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "                                                            Y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy of MNIST Train Data:\", accuracy.eval({X: mnist.train.images, Y: mnist.train.labels}))\n",
    "    print(\"Accuracy of MNIST Validation Data:\", accuracy.eval({X: mnist.validation.images, Y: mnist.validation.labels}))\n",
    "    print(\"Accuracy of MNIST Test Data:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    print(\"Accuracy for USPS Numerals:\", accuracy.eval({X: USPS_img_array, Y: USPS_label_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_my_usps_data(): \n",
    "    import zipfile\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    import PIL.ImageOps  \n",
    "    import numpy as np\n",
    "    import tensorflow  as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    filename=\"usps_dataset_handwritten.zip\"\n",
    "\n",
    "    #Defining height,width for resizing the images to 28x28 like MNIST digits\n",
    "    height=28\n",
    "    width=28\n",
    "\n",
    "    #Defining path for extracting dataset zip file\n",
    "    extract_path = \"usps_data\"\n",
    "\n",
    "    #Defining image,label list\n",
    "    images = []\n",
    "    img_list = []\n",
    "    labels = []\n",
    "\n",
    "    #Extracting given dataset file    \n",
    "    with zipfile.ZipFile(filename, 'r') as zip:\n",
    "        zip.extractall(extract_path)\n",
    "\n",
    "    #Extracting labels,images array needed for training    \n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        if \"Numerals\" in path:\n",
    "            image_files = [fname for fname in files if fname.find(\".png\") >= 0]\n",
    "            for file in image_files:\n",
    "                labels.append(int(path[-1]))\n",
    "                images.append(os.path.join(*path, file)) \n",
    "\n",
    "    #Resizing images like MNIST dataset   \n",
    "    for idx, imgs in enumerate(images):\n",
    "        img = Image.open(imgs).convert('L') \n",
    "        img = img.resize((height, width), Image.ANTIALIAS)\n",
    "        img_data = list(img.getdata())\n",
    "        img_list.append(img_data)\n",
    "\n",
    "    #Storing image and labels in arrays to be used for training   \n",
    "    USPS_img_array = np.array(img_list)\n",
    "    USPS_img_array = np.subtract(255, USPS_img_array)\n",
    "    USPS_label_array = np.array(labels)\n",
    "    #print(USPS_label_array.shape)\n",
    "    nb_classes = 10\n",
    "    targets = np.array(USPS_label_array).reshape(-1)\n",
    "    aa = np.eye(nb_classes)[targets]\n",
    "    USPS_label_array = np.array(aa, dtype=np.int32)\n",
    "    #print(USPS_label_array)\n",
    "\n",
    "\n",
    "    USPS_img_array = np.float_(np.array(USPS_img_array))\n",
    "    for z in range(len(USPS_img_array)):\n",
    "        USPS_img_array[z] /= 255.0 \n",
    "\n",
    "    plt.imshow(USPS_img_array[19998].reshape(28,28))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return USPS_img_array, USPS_label_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSkAMaZMVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1927fc29e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "USPS_img_array, USPS_label_array = get_my_usps_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
